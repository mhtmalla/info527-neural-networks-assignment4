{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1Ak4If2D-6M",
        "outputId": "649c90dd-81d2-4c9b-a75c-e0c8bff87676"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load nn.py"
      ],
      "metadata": {
        "id": "8KraeMgbELz0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('nn.py', 'r') as file:\n",
        "    code = file.read()"
      ],
      "metadata": {
        "id": "GMDGVG7wEL81"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tUHxeRREMF7",
        "outputId": "e8bac434-0416-42e0-d373-c60c1f103ec2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"nn.ipynb\n",
            "\n",
            "Automatically generated by Colab.\n",
            "\n",
            "Original file is located at\n",
            "    https://colab.research.google.com/drive/1-VRE0jy4Xz9RhYiM2dha7lromWH9wH3n\n",
            "\"\"\"\n",
            "\n",
            "\"\"\"\n",
            "The main code for the recurrent and convolutional networks assignment.\n",
            "See README.md for details.\n",
            "\"\"\"\n",
            "from typing import Tuple, List, Dict\n",
            "import tensorflow\n",
            "import tensorflow as tf\n",
            "from tensorflow.keras.losses import MeanSquaredError\n",
            "from tensorflow.keras import regularizers\n",
            "\n",
            "\n",
            "def create_toy_rnn(input_shape: tuple, n_outputs: int) \\\n",
            "        -> Tuple[tensorflow.keras.models.Model, Dict]:\n",
            "    \"\"\"Creates a recurrent neural network for a toy problem.\n",
            "\n",
            "    The network will take as input a sequence of number pairs, (x_{t}, y_{t}),\n",
            "    where t is the time step. It must learn to produce x_{t-3} - y{t} as the\n",
            "    output of time step t.\n",
            "\n",
            "    This method does not call Model.fit, but the dictionary it returns alongside\n",
            "    the model will be passed as extra arguments whenever Model.fit is called.\n",
            "    This can be used to, for example, set the batch size or use early stopping.\n",
            "\n",
            "    :param input_shape: The shape of the inputs to the model.\n",
            "    :param n_outputs: The number of outputs from the model.\n",
            "    :return: A tuple of (neural network, Model.fit keyword arguments)\n",
            "    \"\"\"\n",
            "    ### YOUR CODE HERE ###\n",
            "\n",
            "    # Define the input layer with the specified input shape\n",
            "    input_layer = tf.keras.Input(shape=input_shape)\n",
            "\n",
            "    # First LSTM layer: 256 units (neurons)\n",
            "    x = tf.keras.layers.LSTM(256, return_sequences=True, activation='tanh')(input_layer) # 'tanh' activation function\n",
            "    # Apply dropout to prevent overfitting (drop 30% of the units)\n",
            "    x = tf.keras.layers.Dropout(0.3)(x)\n",
            "\n",
            "    # Second LSTM layer: 128 units\n",
            "    x = tf.keras.layers.LSTM(128, return_sequences=True, activation='tanh')(x)\n",
            "    # Apply dropout\n",
            "    x = tf.keras.layers.Dropout(0.3)(x)\n",
            "\n",
            "    # Third LSTM layer: 64 units\n",
            "    x = tf.keras.layers.LSTM(64, return_sequences=True, activation='tanh')(x)\n",
            "    # Apply dropout\n",
            "    x = tf.keras.layers.Dropout(0.3)(x)\n",
            "\n",
            "    # Output layer: Dense layer with 'linear' activation for regression output\n",
            "    output_layer = tf.keras.layers.Dense(\n",
            "        n_outputs,\n",
            "        activation='linear',\n",
            "        kernel_regularizer=regularizers.l2(0.001) # L2 regularization applied to the kernel weights to reduce overfitting\n",
            "    )(x)\n",
            "\n",
            "    # Create the model specifying the inputs and outputs\n",
            "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
            "\n",
            "    # Compile the model: Use RMSprop optimizer with a small learning rate and gradient clipping\n",
            "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001, clipnorm=1.0)\n",
            "    model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=['mae'])\n",
            "\n",
            "    # Define training arguments:\n",
            "    fit_args = {\n",
            "        'batch_size': 4, # Small batch size for more frequent weight updates\n",
            "        'epochs': 100, # Increased number of epochs to allow the model to learn sufficiently\n",
            "        'callbacks': [\n",
            "            tf.keras.callbacks.EarlyStopping(\n",
            "                monitor='val_loss',\n",
            "                patience=15, # EarlyStopping callback to prevent overfitting\n",
            "                restore_best_weights=True\n",
            "            )\n",
            "        ]\n",
            "    }\n",
            "\n",
            "    # Return the compiled model and the training arguments\n",
            "    return model, fit_args\n",
            "\n",
            "def create_mnist_cnn(input_shape: tuple, n_outputs: int) \\\n",
            "        -> Tuple[tensorflow.keras.models.Model, Dict]:\n",
            "    \"\"\"Creates a convolutional neural network for digit classification.\n",
            "\n",
            "    The network will take as input a 28x28 grayscale image, and produce as\n",
            "    output one of the digits 0 through 9. The network will be trained and tested\n",
            "    on a fraction of the MNIST data: http://yann.lecun.com/exdb/mnist/\n",
            "\n",
            "    This method does not call Model.fit, but the dictionary it returns alongside\n",
            "    the model will be passed as extra arguments whenever Model.fit is called.\n",
            "    This can be used to, for example, set the batch size or use early stopping.\n",
            "\n",
            "    :param input_shape: The shape of the inputs to the model.\n",
            "    :param n_outputs: The number of outputs from the model.\n",
            "    :return: A tuple of (neural network, Model.fit keyword arguments)\n",
            "    \"\"\"\n",
            "    ### YOUR CODE HERE ###\n",
            "\n",
            "    # Input layer for 28x28 grayscale images\n",
            "    mnist_input = tf.keras.Input(shape=input_shape)\n",
            "\n",
            "    # First convolutional layer with 32 filters and a 3x3 kernel\n",
            "    conv1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(mnist_input)\n",
            "\n",
            "    # MaxPooling to reduce spatial dimensions\n",
            "    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
            "\n",
            "    # Second convolutional layer with 64 filters and a 3x3 kernel\n",
            "    conv2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
            "\n",
            "    # MaxPooling to further reduce spatial dimensions\n",
            "    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
            "\n",
            "    # Flatten the output from 2D to 1D for fully connected layers\n",
            "    flat_layer = tf.keras.layers.Flatten()(pool2)\n",
            "\n",
            "    # Dense layer with 128 units for feature learning\n",
            "    dense_layer = tf.keras.layers.Dense(128, activation='relu')(flat_layer)\n",
            "\n",
            "    # Output layer with 'softmax' activation for classification into 10 digits\n",
            "    output_layer = tf.keras.layers.Dense(n_outputs, activation='softmax')(dense_layer)\n",
            "\n",
            "    # Create the model\n",
            "    mnist_cnn_model = tf.keras.Model(inputs=mnist_input, outputs=output_layer)\n",
            "\n",
            "    # Compile the model with categorical crossentropy for classification\n",
            "    mnist_cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
            "\n",
            "    # Arguments for model training (e.g., batch size, early stopping).\n",
            "    fit_args = {\n",
            "        'batch_size': 64,  # Batch size for training\n",
            "        'callbacks': [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]  # Early stopping\n",
            "    }\n",
            "\n",
            "    # Return the model and training arguments\n",
            "    return mnist_cnn_model, fit_args\n",
            "\n",
            "def create_youtube_comment_rnn(vocabulary: List[str], n_outputs: int) \\\n",
            "        -> Tuple[tensorflow.keras.models.Model, Dict]:\n",
            "    \"\"\"Creates a recurrent neural network for spam classification.\n",
            "\n",
            "    This network will take as input a YouTube comment, and produce as output\n",
            "    either 1, for spam, or 0, for ham (non-spam). The network will be trained\n",
            "    and tested on data from:\n",
            "    https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection\n",
            "\n",
            "    Each comment is represented as a series of tokens, with each token\n",
            "    represented by a number, which is its index in the vocabulary. Note that\n",
            "    comments may be of variable length, so in the input matrix, comments with\n",
            "    fewer tokens than the matrix width will be right-padded with zeros.\n",
            "\n",
            "    This method does not call Model.fit, but the dictionary it returns alongside\n",
            "    the model will be passed as extra arguments whenever Model.fit is called.\n",
            "    This can be used to, for example, set the batch size or use early stopping.\n",
            "\n",
            "    :param vocabulary: The vocabulary defining token indexes.\n",
            "    :param n_outputs: The number of outputs from the model.\n",
            "    :return: A tuple of (neural network, Model.fit keyword arguments)\n",
            "    \"\"\"\n",
            "    ### YOUR CODE HERE ###\n",
            "\n",
            "    # Size of the vocabulary (number of unique tokens)\n",
            "    vocab_size = len(vocabulary)\n",
            "    # Dimension for word embeddings (each word will be represented by a 300-dimensional vector)\n",
            "    embedding_dim = 300\n",
            "    # Maximum length of a comment (number of tokens).\n",
            "    max_comment_length = 200\n",
            "\n",
            "    # Define the input layer\n",
            "    youtube_input = tf.keras.Input(shape=(max_comment_length,), dtype='int32')\n",
            "\n",
            "    # Embedding layer\n",
            "    embedding_layer = tf.keras.layers.Embedding(\n",
            "        input_dim=vocab_size,          # Size of the vocabulary\n",
            "        output_dim=embedding_dim       # Dimension of the embedding vectors\n",
            "    )(youtube_input)\n",
            "\n",
            "    # First LSTM layer\n",
            "    lstm_layer = tf.keras.layers.LSTM(\n",
            "        128, # Number of LSTM cells\n",
            "        return_sequences=True,\n",
            "        activation='tanh' # Activation function for the LSTM cells\n",
            "    )(embedding_layer)\n",
            "\n",
            "    # Bidirectional LSTM layer (processes the sequence in both forward and backward directions)\n",
            "    bi_lstm_layer = tf.keras.layers.Bidirectional(\n",
            "        tf.keras.layers.LSTM(\n",
            "            128, # - 128 units for each direction\n",
            "            return_sequences=False,\n",
            "            activation='tanh'\n",
            "        )\n",
            "    )(lstm_layer)\n",
            "\n",
            "    # Dense layer with 64 neurons and ReLU activation\n",
            "    dense_layer = tf.keras.layers.Dense(\n",
            "        64,\n",
            "        activation='relu'\n",
            "    )(bi_lstm_layer)\n",
            "\n",
            "    # Output layer with sigmoid activation for binary classification\n",
            "    # Outputs a probability between 0 and 1 indicating spam likelihood\n",
            "    output_layer = tf.keras.layers.Dense(\n",
            "        n_outputs,\n",
            "        activation='sigmoid'\n",
            "    )(dense_layer)\n",
            "\n",
            "    # Create the Keras model\n",
            "    model = tf.keras.Model(inputs=youtube_input, outputs=output_layer)\n",
            "\n",
            "    # Compile the model\n",
            "    model.compile(\n",
            "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), # Adam with a low learning rate for fine-tuned weight updates\n",
            "        loss='binary_crossentropy', # Binary Crossentropy for binary classification tasks\n",
            "        metrics=['accuracy']\n",
            "    )\n",
            "\n",
            "    # Additional arguments for model training\n",
            "    fit_args = {\n",
            "        'batch_size': 32, # batch_size\n",
            "        'epochs': 20, # epochs i.e Number of complete passes through the training dataset\n",
            "        'callbacks': [\n",
            "            tf.keras.callbacks.EarlyStopping(\n",
            "                monitor='val_loss',    # Monitor the validation loss\n",
            "                patience=5,            # Number of epochs with no improvement after which training will be stopped\n",
            "                restore_best_weights=True\n",
            "            )\n",
            "        ]\n",
            "    }\n",
            "\n",
            "    # Return the compiled model and the training arguments\n",
            "    return model, fit_args\n",
            "\n",
            "def create_youtube_comment_cnn(vocabulary: List[str], n_outputs: int) \\\n",
            "        -> Tuple[tensorflow.keras.models.Model, Dict]:\n",
            "    \"\"\"Creates a convolutional neural network for spam classification.\n",
            "\n",
            "    This network will take as input a YouTube comment, and produce as output\n",
            "    either 1, for spam, or 0, for ham (non-spam). The network will be trained\n",
            "    and tested on data from:\n",
            "    https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection\n",
            "\n",
            "    Each comment is represented as a series of tokens, with each token\n",
            "    represented by a number, which is its index in the vocabulary. Note that\n",
            "    comments may be of variable length, so in the input matrix, comments with\n",
            "    fewer tokens than the matrix width will be right-padded with zeros.\n",
            "\n",
            "    This method does not call Model.fit, but the dictionary it returns alongside\n",
            "    the model will be passed as extra arguments whenever Model.fit is called.\n",
            "    This can be used to, for example, set the batch size or use early stopping.\n",
            "\n",
            "    :param vocabulary: The vocabulary defining token indexes.\n",
            "    :param n_outputs: The number of outputs from the model.\n",
            "    :return: A tuple of (neural network, Model.fit keyword arguments)\n",
            "    \"\"\"\n",
            "    ### YOUR CODE HERE ###\n",
            "\n",
            "    vocab_size = len(vocabulary)  # Number of unique words in vocabulary\n",
            "    embedding_dim = 128  # Dimension of word embeddings\n",
            "    max_comment_length = 200  # Maximum length of comments\n",
            "\n",
            "    # Input layer for tokenized comments\n",
            "    youtube_input = tf.keras.Input(shape=(max_comment_length,), dtype='int32')\n",
            "\n",
            "    # Embedding layer to convert tokens into dense vectors\n",
            "    embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(youtube_input)\n",
            "\n",
            "    # Convolutional layer with ReLU activation\n",
            "    conv_layer = tf.keras.layers.Conv1D(128, 5, activation='relu')(embedding_layer)\n",
            "\n",
            "    # MaxPooling layer to reduce the size of the feature map\n",
            "    max_pooling_layer = tf.keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
            "\n",
            "    # Dense layer for classification\n",
            "    dense_layer = tf.keras.layers.Dense(64, activation='relu')(max_pooling_layer)\n",
            "    output_layer = tf.keras.layers.Dense(n_outputs, activation='sigmoid')(dense_layer)\n",
            "\n",
            "    # Create and compile the CNN model\n",
            "    model = tf.keras.Model(inputs=youtube_input, outputs=output_layer)\n",
            "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
            "\n",
            "    # Additional arguments for Model.fit\n",
            "    fit_args = {\n",
            "        'batch_size': 64, # Batch size\n",
            "        'callbacks': [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]  # Stop early if no improvement\n",
            "    }\n",
            "\n",
            "    return model, fit_args\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load test_nn.py"
      ],
      "metadata": {
        "id": "PMsdZXk3EMJ9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPV8uEa_EMNt",
        "outputId": "5e4d595a-c9d8-4517-d8bf-388ffbf2d744"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1, typeguard-4.3.0\n",
            "collected 4 items                                                                                  \u001b[0m\n",
            "\n",
            "test_nn.py \n",
            "0.4 RMSE for RNN on toy problem\n",
            "\u001b[32m.\u001b[0m\n",
            "88.0% accuracy for CNN on MNIST sample\n",
            "\u001b[32m.\u001b[0m\n",
            "84.6% accuracy for RNN on Youtube comments\n",
            "\u001b[32m.\u001b[0m\n",
            "87.0% accuracy for CNN on Youtube comments\n",
            "\u001b[32m.\u001b[0m\u001b[32m                                                                              [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m================================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 221.04s (0:03:41)\u001b[0m\u001b[32m ===================================\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}